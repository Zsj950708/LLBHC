{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# require necessary packages\n",
    "from scipy import special\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit, vectorize, float64, int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class node():\n",
    "    \"\"\"\n",
    "    The class for node.\n",
    "    Initialization requires two child branches, which may be None for end node, \n",
    "    the data points, and the index number of cluter.\n",
    "    When initialized, the node also keeps track of the number of total nodes,\n",
    "    both internal and end, within its structure as n.\n",
    "    \n",
    "    For example:\n",
    "    new_node = node(None, None, np.repeat(1,4), 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialization \n",
    "    def __init__(self, left, right, data, cnum):\n",
    "        # set up the left and right branches\n",
    "        self.l = left\n",
    "        self.r = right\n",
    "        \n",
    "        # track of number of total nodes using recursion\n",
    "        if(left == None and right == None):\n",
    "            self.n = 1\n",
    "        else:\n",
    "            self.n = left.n + right.n\n",
    "            \n",
    "        # save data points and number of cluster\n",
    "        self.data = data\n",
    "        self.cluster = cnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@jit(float64(float64[:,:], int64[:]))\n",
    "\n",
    "def p_hyp1(dataset, a):\n",
    "    \"\"\"\n",
    "    Function to calculate the posterior probability.\n",
    "    The input requires data, which is either a vector\n",
    "    or a 2D numpy array, and an alpha value, which is\n",
    "    a double\n",
    "    \"\"\"\n",
    "    \n",
    "    # extract the number of features and the total number of data\n",
    "    \n",
    "    # If the data is a vector, do the following\n",
    "    if (len(dataset.shape) == 1):\n",
    "        N = 1\n",
    "        k = dataset.shape[0]\n",
    "        # part I\n",
    "        p1 = 1\n",
    "        comp = special.gamma(np.sum(dataset)+1) / np.prod(special.gamma(dataset+1))\n",
    "        p1 = p1 * comp\n",
    "        \n",
    "        # part II\n",
    "        # iterate to calculate the probability\n",
    "        p2 = p1 * special.gamma(np.sum(a)) / special.gamma(np.sum(dataset) + np.sum(a))\n",
    "        for j in range(k):\n",
    "            comp = special.gamma(a[j] + np.sum(dataset[j])) / special.gamma(a[j])\n",
    "            p2 = p2 * comp\n",
    "    # if the data is not vector, do the following\n",
    "    else:\n",
    "        N = dataset.shape[0]\n",
    "        k = dataset.shape[1]\n",
    "    \n",
    "        # part I\n",
    "        p1 = 1\n",
    "        for i in range(N):\n",
    "            comp = special.gamma(np.sum(dataset[i, :])+1) / np.prod(special.gamma(dataset[i, :]+1))\n",
    "            p1 = p1 * comp\n",
    "        \n",
    "        # part II\n",
    "        # iterate to calculate the probability\n",
    "        p2 = p1 * special.gamma(np.sum(a)) / special.gamma(np.sum(dataset) + np.sum(a))\n",
    "        for j in range(k):\n",
    "            comp = special.gamma(a[j] + np.sum(dataset[:, j])) / special.gamma(a[j])\n",
    "            p2 = p2 * comp\n",
    "\n",
    "    return p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_d(node, a):\n",
    "    \"\"\"\n",
    "    Recursive function to calculate the 'd' value for each node\n",
    "    \n",
    "    \"\"\"\n",
    "    if node.l == None and node.r == None:\n",
    "        return a\n",
    "    else:\n",
    "        return a*special.gamma(node.n) + get_d(node.l, a)*get_d(node.r, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pi(node, a):\n",
    "    \"\"\"\n",
    "    The function to calculate the weight for each node (pi_k).\n",
    "    It uses d and the gamma function.\n",
    "    The inputs are a node object and a double\n",
    "    \"\"\"\n",
    "    dk = get_d(node, a)\n",
    "    pi_k = a*special.gamma(node.n)/dk\n",
    "    return pi_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dk(node, a):\n",
    "    \"\"\"\n",
    "    The Recursive function to calculate the posterior probability for\n",
    "    each node given a subtree (Ti).\n",
    "    The inputs are a node and a double\n",
    "    \"\"\"\n",
    "    post = p_hyp1(node.data, np.repeat(a, node.data.shape[1]))\n",
    "    pi = get_pi(node, a)\n",
    "    if node.l == None and node.r == None:\n",
    "        return  pi * post\n",
    "    else:\n",
    "        return  pi * post + (1-pi) * get_dk(node.l, a) * get_dk(node.r, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  1],\n",
       "       [ 0,  1,  0],\n",
       "       [ 0,  1,  0],\n",
       "       [ 1,  0,  1],\n",
       "       [ 1,  1,  0],\n",
       "       [10,  9, 10],\n",
       "       [10,  9, 10],\n",
       "       [10, 10, 10],\n",
       "       [ 9, 10, 10],\n",
       "       [ 9,  9, 10],\n",
       "       [16, 17, 17],\n",
       "       [17, 17, 16],\n",
       "       [16, 17, 16],\n",
       "       [17, 16, 16],\n",
       "       [16, 16, 17]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## simulated test data\n",
    "d1 = np.random.randint(0,2, size=(5,3))\n",
    "d2 = np.random.randint(9,11, size=(5,3))\n",
    "d3 = np.random.randint(16,18, size=(5,3))\n",
    "sdata = np.concatenate((d1,d2,d3),axis=0)\n",
    "sdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"wine.csv\") as f:\n",
    "    next(f)\n",
    "    text = f.read() \n",
    "\n",
    "data = []\n",
    "lines  = text.split('\\n')\n",
    "for line in lines[:-1]:\n",
    "    arr = line.split(';')\n",
    "    fl = [int(np.round(float(x))) for x in arr]\n",
    "    data.append(fl)\n",
    "\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  1,  0,  2,  0],\n",
       "       [ 8,  1,  0,  3,  0],\n",
       "       [ 8,  1,  0,  2,  0],\n",
       "       [11,  0,  1,  2,  0],\n",
       "       [ 7,  1,  0,  2,  0],\n",
       "       [ 7,  1,  0,  2,  0],\n",
       "       [ 8,  1,  0,  2,  0],\n",
       "       [ 7,  1,  0,  1,  0],\n",
       "       [ 8,  1,  0,  2,  0],\n",
       "       [ 8,  0,  0,  6,  0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test data from wine.csv\n",
    "tdata = data[:10,:5]\n",
    "tdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bhc(data, a=1, r_thres=0.5):\n",
    "    \"\"\"\n",
    "    The Baysian Hierarchical Clustering algorithm.\n",
    "    It is described in the paper collaborated by\n",
    "    Dr. Katherine Heller and Dr. Zoubin Ghahramani\n",
    "    in 2005.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a node_list tracking the nodes to be merged\n",
    "    # and a node_list_copy to track the cluster number of each\n",
    "    # node. The initial value of those two lists are each data\n",
    "    # points with its own cluster number.\n",
    "    node_list = []\n",
    "    node_list_copy = []\n",
    "    for i in range(data.shape[0]):\n",
    "        node_list.append(node(None, None, np.array([data[i,:]]), i))\n",
    "        node_list_copy.append(node(None, None, np.array([data[i,:]]), i))\n",
    "    \n",
    "    # Cluster number, default value equals the number of data points\n",
    "    c = data.shape[0]\n",
    "    \n",
    "    # Iterate to merge nodes. Note that BHC is a greedy algorithm, which means\n",
    "    # If no tow nodes can be merged, the loop stops automatically\n",
    "    while c > 1:\n",
    "        # Indicate whether to break the while loop\n",
    "        flag = False\n",
    "        \n",
    "        for i in range(len(node_list)):\n",
    "            for j in range(i+1, len(node_list)):\n",
    "                # Create a new data by row-binding the datasets in the two nodes\n",
    "                newdata = np.concatenate((node_list[i].data, node_list[j].data), axis = 0)\n",
    "                \n",
    "                # Create a new node based on the new data\n",
    "                # Set the cluster number of the new node to\n",
    "                # the minimum of the two nodes combined\n",
    "                node_new = node(node_list[i], node_list[j], newdata, \n",
    "                                min(node_list[i].cluster,node_list[j].cluster))\n",
    "                \n",
    "                # Calculate the probability of the hypothesis being true\n",
    "                pi_k = get_pi(node_new, a)\n",
    "                \n",
    "                # Calculate the posterior probability of data given hypothesis\n",
    "                p_hyp = p_hyp1(node_new.data, np.repeat(a, data.shape[1]))\n",
    "                \n",
    "                # Calculate the posterior probability of data given subtree\n",
    "                p_dk = get_dk(node_new, a)\n",
    "                \n",
    "                # Calculate the probability of the merged hypothesis\n",
    "                rk = pi_k * p_hyp / p_dk\n",
    "                \n",
    "                # If the probability of the merged hypothesis is greater\n",
    "                # than the threshold we set, merge the two nodes, reset\n",
    "                # their cluster number in node_list_copy and remove the\n",
    "                # two nodes from nodes_list.\n",
    "                \n",
    "                # Note that since it's a greedy algorithm, we break the\n",
    "                # double for loop if the nodes are merged and continue\n",
    "                # on finding the next two nodes to merge.\n",
    "                if rk >= r_thres:\n",
    "                    for k in range(len(node_list_copy)):\n",
    "                        entry = node_list_copy[k].cluster\n",
    "                        if entry == node_list[i].cluster or entry == node_list[j].cluster:\n",
    "                            node_list_copy[k].cluster = min(node_list[i].cluster,node_list[j].cluster)\n",
    "                    node_list =  node_list[:i] + node_list[(i+1):j] + node_list[(j+1):]\n",
    "                    node_list = [node_new] + node_list\n",
    "                    \n",
    "                    c = c - 1\n",
    "                    flag = True\n",
    "                    break\n",
    "            if flag == True:\n",
    "                break\n",
    "        \n",
    "        if flag == False:\n",
    "            c = 1        \n",
    "\n",
    "    return node_list, node_list_copy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test when the threshold for rk is 0 and alpha = 1\n",
    "node_list, node_list_cluster = bhc(tdata, a=1, r_thres=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[node.cluster for node in node_list_cluster]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 7,  1,  0,  2,  0],\n",
       "        [ 8,  1,  0,  3,  0],\n",
       "        [ 8,  1,  0,  2,  0],\n",
       "        [11,  0,  1,  2,  0],\n",
       "        [ 7,  1,  0,  2,  0],\n",
       "        [ 7,  1,  0,  2,  0],\n",
       "        [ 8,  1,  0,  2,  0],\n",
       "        [ 7,  1,  0,  1,  0],\n",
       "        [ 8,  1,  0,  2,  0],\n",
       "        [ 8,  0,  0,  6,  0]])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[node.data for node in node_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test when the threshold for rk is 1 and alpha = 1\n",
    "nodes, nodes_cluster = bhc(tdata, a=1, r_thres=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[node.cluster for node in nodes_cluster]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[7, 1, 0, 2, 0]]),\n",
       " array([[8, 1, 0, 3, 0]]),\n",
       " array([[8, 1, 0, 2, 0]]),\n",
       " array([[11,  0,  1,  2,  0]]),\n",
       " array([[7, 1, 0, 2, 0]]),\n",
       " array([[7, 1, 0, 2, 0]]),\n",
       " array([[8, 1, 0, 2, 0]]),\n",
       " array([[7, 1, 0, 1, 0]]),\n",
       " array([[8, 1, 0, 2, 0]]),\n",
       " array([[8, 0, 0, 6, 0]])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[node.data for node in nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nodes_sim, nodes_cluster_sim = bhc(sdata, a=1, r_thres=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 10, 10, 13, 13]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[node.cluster for node in nodes_cluster_sim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def p_hyp1(dataset, a):\n",
    "    \"\"\"\n",
    "    Function to calculate the posterior probability.\n",
    "    The input requires data, which is either a vector\n",
    "    or a 2D numpy array, and an alpha value, which is\n",
    "    a double\n",
    "    \"\"\"\n",
    "    \n",
    "    # extract the number of features and the total number of data\n",
    "    \n",
    "    # If the data is a vector, do the following\n",
    "    if (len(dataset.shape) == 1):\n",
    "        N = 1\n",
    "        k = dataset.shape[0]\n",
    "        # part I\n",
    "        p1 = 1\n",
    "        comp = special.gamma(np.sum(dataset)+1) / np.prod(special.gamma(dataset+1))\n",
    "        p1 = p1 * comp\n",
    "        \n",
    "        # part II\n",
    "        # iterate to calculate the probability\n",
    "        p2 = p1 * special.gamma(np.sum(a)) / special.gamma(np.sum(dataset) + np.sum(a))\n",
    "        for j in range(k):\n",
    "            comp = special.gamma(a[j] + np.sum(dataset[j])) / special.gamma(a[j])\n",
    "            p2 = p2 * comp\n",
    "    # if the data is not vector, do the following\n",
    "    else:\n",
    "        N = dataset.shape[0]\n",
    "        k = dataset.shape[1]\n",
    "    \n",
    "        # part I\n",
    "        p1 = 1\n",
    "        for i in range(N):\n",
    "            comp = special.gamma(np.sum(dataset[i, :])+1) / np.prod(special.gamma(dataset[i, :]+1))\n",
    "            p1 = p1 * comp\n",
    "        \n",
    "        # part II\n",
    "        # iterate to calculate the probability\n",
    "        p2 = p1 * special.gamma(np.sum(a)) / special.gamma(np.sum(dataset) + np.sum(a))\n",
    "        for j in range(k):\n",
    "            comp = special.gamma(a[j] + np.sum(dataset[:, j])) / special.gamma(a[j])\n",
    "            p2 = p2 * comp\n",
    "\n",
    "    return p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 7.09 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1 loop, best of 3: 101 ms per loop\n"
     ]
    }
   ],
   "source": [
    "# Without jit\n",
    "%timeit bhc(sdata, a=1, r_thres=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@jit(float64(float64[:,:], int64[:]))\n",
    "\n",
    "def p_hyp1(dataset, a):\n",
    "    \"\"\"\n",
    "    Function to calculate the posterior probability.\n",
    "    The input requires data, which is either a vector\n",
    "    or a 2D numpy array, and an alpha value, which is\n",
    "    a double\n",
    "    \"\"\"\n",
    "    \n",
    "    # extract the number of features and the total number of data\n",
    "    \n",
    "    # If the data is a vector, do the following\n",
    "    if (len(dataset.shape) == 1):\n",
    "        N = 1\n",
    "        k = dataset.shape[0]\n",
    "        # part I\n",
    "        p1 = 1\n",
    "        comp = special.gamma(np.sum(dataset)+1) / np.prod(special.gamma(dataset+1))\n",
    "        p1 = p1 * comp\n",
    "        \n",
    "        # part II\n",
    "        # iterate to calculate the probability\n",
    "        p2 = p1 * special.gamma(np.sum(a)) / special.gamma(np.sum(dataset) + np.sum(a))\n",
    "        for j in range(k):\n",
    "            comp = special.gamma(a[j] + np.sum(dataset[j])) / special.gamma(a[j])\n",
    "            p2 = p2 * comp\n",
    "    # if the data is not vector, do the following\n",
    "    else:\n",
    "        N = dataset.shape[0]\n",
    "        k = dataset.shape[1]\n",
    "    \n",
    "        # part I\n",
    "        p1 = 1\n",
    "        for i in range(N):\n",
    "            comp = special.gamma(np.sum(dataset[i, :])+1) / np.prod(special.gamma(dataset[i, :]+1))\n",
    "            p1 = p1 * comp\n",
    "        \n",
    "        # part II\n",
    "        # iterate to calculate the probability\n",
    "        p2 = p1 * special.gamma(np.sum(a)) / special.gamma(np.sum(dataset) + np.sum(a))\n",
    "        for j in range(k):\n",
    "            comp = special.gamma(a[j] + np.sum(dataset[:, j])) / special.gamma(a[j])\n",
    "            p2 = p2 * comp\n",
    "\n",
    "    return p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 39.5 ms per loop\n"
     ]
    }
   ],
   "source": [
    "# with jit\n",
    "%timeit bhc(sdata, a=1, r_thres=0.5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
